{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of different models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    S. No |  Model                |    Training Data    |    Test Data\n",
    "    \n",
    "    1.    |  SVM Regressor        |    0.9648315        |    0.9562091\n",
    "    2.    |  Linear Regression    |    0.9726291        |    0.9660127\n",
    "    3.    |  Decision Tree        |    0.9922471        |    0.9665272\n",
    "    4.    |  ADA Boost Regressor  |    0.9742539        |    0.9745007\n",
    "    5.    |  Random Forest        |    0.9966556        |    0.9774058\n",
    "    6.    |  Neural Network       |    0.9884488        |    0.9812295\n",
    "    7.    |  GB Regressor         |    0.9998022        |    0.9792305\n",
    "    8.    |  LightGBM Regressor   |    0.9975067        |    0.9847313\n",
    "    9.    |  XGBoost Regressor    |    0.9986248        |    0.9830212\n",
    "    \n",
    "    10.   |  Ensemble model       |    0.9871900        |    0.9730434"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance derived from various models:\n",
    "\n",
    "\n",
    "Features gathered from the observation of a phenomenon are not all equally informative: some of them may be noisy, correlated or irrelevant. Feature selection aims at selecting a feature set that is relevant for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    S. No |  Model               |  Rank 1  |  Rank 2  |  Rank 3  |  Rank 4  |  Rank 5 \n",
    "    \n",
    "    1.    |  Decision Tree       |  Dt      |  Cr      |  C       |  TT      |  NT       \n",
    "    2.    |  ADA Boost Regressor |  TT      |  Cr      |  Tt      |  Dt      |  Ct\n",
    "    3.    |  Random Forest       |  QmT     |  Ct      |  Cr      |  NT      |  Tt\n",
    "    4.    |  LightGBM Regressor  |  TT      |  C       |  Cr      |  P       |  Mn\n",
    "    5.    |  XGBoost Regressor   |  TT      |  C       |  Cr      |  Mn      |  P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"/Users/chiragbhattad/Downloads/DDP/Fatigue Strength dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset using pandas library and dropping the 'SI. No.' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"fatigue.xlsx\")\n",
    "data.drop(['Sl. No.'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data correlation visualized using heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "fig = plt.figure(figsize=(25,25))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(data.columns),1)\n",
    "ax.set_xticks(ticks)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(data.columns)\n",
    "ax.set_yticklabels(data.columns)\n",
    "plt.show()\n",
    "fig.savefig('corr_heat_map.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting of the dataset into feature variable and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = data.columns[:-1]\n",
    "target = data.columns[-1]\n",
    "X = data[feature]\n",
    "y = data[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['TT'], data['Fatigue'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.title('Tempering temperature vs Fatigue strength')\n",
    "plt.xlabel('Tempering temperature')\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "plt.savefig('TTvsFatigueStrength.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data['CT']\n",
    "color= ['blue' if l == 30 else 'orange' for l in label]\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.scatter(data['C'], data['Fatigue'], color=color)\n",
    "plt.title('Scatter plot categorized by carburizing temperature')\n",
    "plt.xlabel('% Carbon')\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "plt.savefig('CarbonvsFatigueStrength.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data['CT']\n",
    "color= ['blue' if l == 30 else 'orange' for l in label]\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.scatter(data['P'], data['Fatigue'], color = color)\n",
    "plt.title('Scatter plot categorized by carburizing temperature')\n",
    "plt.xlabel('% Phosporous')\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "plt.savefig('PhosporousvsFatigueStrength.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data['CT']\n",
    "color= ['blue' if l == 30 else 'orange' for l in label]\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.scatter(data['Mn'], data['Fatigue'], color = color)\n",
    "plt.title('Scatter plot categorized by carburizing temperature')\n",
    "plt.xlabel('% Manganese')\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "plt.savefig('ManganesevsFatigueStrength.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data['CT']\n",
    "color= ['blue' if l == 30 else 'orange' for l in label]\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.scatter(data['Cr'], data['Fatigue'], color = color)\n",
    "plt.title('Scatter plot categorized by carburizing temperature')\n",
    "plt.xlabel('% Chromium')\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "plt.savefig('ChromiumvsFatigueStrength.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Univariate Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Univariate analysis examines the relationship of each feature with the target variable individually. This can be measured using Pearson coefficient, Maximal Information Coefficient or distance correlation.\n",
    "\n",
    "    1. Pearson Coefficient: It is a measure of the correlation between two variables. It has a value between -1 and +1 with -1 being perfect negative correlation and +1 being perfect positive correlation. Pearson correlation of 0 does not mean the variables are independent.\n",
    "                        ρ(x,y) = cov(X,Y)/σxσy\n",
    "\n",
    "    2. Maximal Information Coefficient: It measures the mutual dependance between two variables. MIC gives a score in bits which is not normalized. It is also inconvenient to compute it for continuous variables  in general the variables need to be discretized by binning, but the mutual information score can be quite sensitive to bin selection.\n",
    "                        I(X,Y) = ∑∑p(x,y)log[p(x,y)/p(x)p(y)]\n",
    "    \n",
    "    3. Distance correlation: It is a measure of dependence between two paired random vectors of arbitrary, not necessarily equal, dimension. Thus, distance correlation measures both linear and nonlinear association between two random variables or random vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, y)\n",
    "np.set_printoptions(precision=3)\n",
    "print(fit.scores_/sum(fit.scores_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected Features:\n",
    "features = fit.transform(X)\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Given an external estimator that assigns weights to features, the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 5)\n",
    "\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "print(\"Num Features: %d\"% fit.n_features_)\n",
    "print(\"Selected Features: %s\"% fit.support_)\n",
    "print(\"Feature Ranking: %s\"% fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance, and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=1)\n",
    "principal_components = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(principal_components[:,0],principal_components[:,1], c = y)\n",
    "plt.title(\"Distribution of training dataset after PCA\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.savefig('pca.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Support Vector Machine can also be used as a regression method, maintaining all the main features that characterize the algorithm (maximal margin). The Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only a few minor differences. First of all, because output is a real number it becomes very difficult to predict the information at hand, which has infinite possibilities. In the case of regression, a margin of tolerance (epsilon) is set in approximation to the SVM which would have already requested from the problem. The main idea is the same: to minimize error, individualizing the hyperplane which maximizes the margin, keeping in mind that part of the error is tolerated.\n",
    "\n",
    "    Hyperplane is the line that will help us predict the continuous value or target value. There are two lines other than Hyper Plane which creates a margin. These are the boundary lines. Support vectors can be on the Boundary lines or outside it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svreg = SVR(kernel='linear', verbose=1, C=15)\n",
    "svreg.fit(X_train, y_train)\n",
    "SVRtrain = r2_score(y_train, svreg.predict(X_train))\n",
    "print(\"\\nSVM Regressor Train data:\", SVRtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_3 = svreg.predict(X_test)\n",
    "SVRerror = r2_score(y_test, Y_pred_3)\n",
    "print(\"SVM Regressor Test data:\", SVRerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the predictions made by the SVM Regressor algorithm and comparing it with the actual data points present in the training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_3 = svreg.predict(X_train)\n",
    "plt.plot(range(349), train_pred_3)\n",
    "plt.plot(range(349), y_train)\n",
    "plt.ylim(0, 1500)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of SVM Regressor on Training data')\n",
    "plt.savefig('SVMRegressorTrain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(88), Y_pred_3)\n",
    "plt.plot(range(88), y_test)\n",
    "plt.ylim(0, 1300)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of SVM Regressor on Test data')\n",
    "plt.savefig('SVMRegressorTest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1200, 1000)\n",
    "plt.plot(x, x+0, '-.k')\n",
    "plt.scatter(y, svreg.predict(X))\n",
    "plt.title(\"SVM Regressor\")\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,1200)\n",
    "plt.savefig('SVMRegressorScatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = svreg.predict(X) - y\n",
    "plt.hist(error, edgecolor='black', linewidth = 1.0, rwidth = 0.9, bins=20)\n",
    "plt.title(\"Error plot for SVM regressor\")\n",
    "plt.xlabel(\"Predicted - Actual (MPa)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.savefig('SVMRegressorError.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Simple linear regression is useful for finding relationship between two continuous variables. One is predictor or independent variable and other is response or dependent variable. It looks for statistical relationship but not deterministic relationship. Relationship between two variables is said to be deterministic if one variable can be accurately expressed by the other. Statistical relationship is not accurate in determining relationship between two variables.\n",
    "\n",
    "    The core idea is to obtain a line that best fits the data. The best fit line is the one for which total prediction error (all data points) are as small as possible. Error is the distance between the point to the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression().fit(X_train,y_train)\n",
    "linregression = linreg.score(X,y)\n",
    "LTerror = r2_score(y_train, linreg.predict(X_train))\n",
    "print(\"Linear Regression Training data:\", LTerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_1 = linreg.predict(X_test)\n",
    "linerror = r2_score(y_test, Y_pred_1)\n",
    "print(\"Linear Regression Test data:\", linerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the predictions made by the Linear regression algorithm and comparing it with the actual data points present in the training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_1 = linreg.predict(X_train)\n",
    "plt.plot(range(349), train_pred_1)\n",
    "plt.plot(range(349), y_train)\n",
    "plt.ylim(0, 1500)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Linear Regression on Training data')\n",
    "plt.savefig('LinRegTrain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(88), Y_pred_1)\n",
    "plt.plot(range(88), y_test)\n",
    "plt.ylim(0, 1300)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Linear Regression on Test data')\n",
    "plt.savefig('LinRegTest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1200, 1000)\n",
    "plt.plot(x, x+0, '-.k')\n",
    "plt.scatter(y, linreg.predict(X))\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,1200)\n",
    "plt.savefig('LinRegScatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_1 = linreg.predict(X) - y\n",
    "plt.hist(error_1, edgecolor='black', linewidth = 1.0, rwidth = 0.9, bins=20)\n",
    "plt.title(\"Error plot for Linear regression\")\n",
    "plt.xlabel(\"Predicted - Actual (MPa)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.savefig('LinRegError.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Polynomial Regression is a form of linear regression in which the relationship between the independent variable x and dependent variable y is modeled as an nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y|x)\n",
    "    \n",
    "    Polynomial Regression are basically used to define or describe non-linear phenomenon such as growth rate of tissues, progression of disease epidemics and distribution of carbon isotopes in lake sediments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree = 3) \n",
    "X_poly = poly.fit_transform(X) \n",
    "  \n",
    "poly.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly = poly.fit_transform(X_train)\n",
    "polreg = LinearRegression().fit(X_train_poly,y_train)\n",
    "# polregression = polreg.score(X_poly,y)\n",
    "PTerror = r2_score(y_train, polreg.predict(X_train_poly))\n",
    "print(\"Polynomial Regression Training data:\", LTerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_poly = poly.fit_transform(X_test)\n",
    "Y_pred_16 = polreg.predict(X_test_poly)\n",
    "polerror = r2_score(y_test, Y_pred_16)\n",
    "print(\"Polynomial Regression Test data:\", linerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_11 = polreg.predict(X_train_poly)\n",
    "plt.plot(range(349), train_pred_11)\n",
    "plt.plot(range(349), y_train)\n",
    "plt.ylim(0, 1500)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Polynomial Regression (degree = 3) on Training data')\n",
    "plt.savefig('PolRegTrain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(88), Y_pred_16)\n",
    "plt.plot(range(88), y_test)\n",
    "plt.ylim(0, 1300)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Polynomial Regression (degree = 3) on Test data')\n",
    "plt.savefig('PolRegTest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1200, 1000)\n",
    "plt.plot(x, x+0, '-.k')\n",
    "plt.scatter(y, polreg.predict(X_poly))\n",
    "plt.title(\"Polynomial Regression (degree = 3)\")\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,1200)\n",
    "plt.savefig('PolRegScatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_1 = polreg.predict(X_poly) - y\n",
    "plt.hist(error_1, edgecolor='black', linewidth = 1.0, rwidth = 0.9, bins=20)\n",
    "plt.title(\"Error plot for Polynomial regression(degree = 3)\")\n",
    "plt.xlabel(\"Predicted - Actual (MPa)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.savefig('PolRegError.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Decision Trees are Simple to understand and to interpret They require very little data preparation.  Thus it can be classified as a white box model i.e., if a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic.\n",
    "\n",
    "    A Decision tree regreesor breaks down the dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision on the numerical target. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtregr = DecisionTreeRegressor(max_depth=7, random_state=1)\n",
    "dtregr.fit(X_train, y_train)\n",
    "DTRtrain = r2_score(y_train, dtregr.predict(X_train))\n",
    "print(\"Decision Tree Train data:\", DTRtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_4 = dtregr.predict(X_test)\n",
    "DTRerror = r2_score(y_test, Y_pred_4)\n",
    "print(\"Decision Tree Test data:\", DTRerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the predictions made by the Decision Tree Regressor and comparing it with the actual data points present in the training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_4 = dtregr.predict(X_train)\n",
    "plt.plot(range(349), train_pred_4)\n",
    "plt.plot(range(349), y_train)\n",
    "plt.ylim(0, 1500)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Decision Tree on Training data')\n",
    "plt.savefig('DTTrain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(88), Y_pred_4)\n",
    "plt.plot(range(88), y_test)\n",
    "plt.ylim(0, 1300)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Decision Tree on Test data')\n",
    "plt.savefig('DTTest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_features = data[feature]\n",
    "importance_dt = dict(zip(independent_features.columns, dtregr.feature_importances_))\n",
    "importance_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1200, 1000)\n",
    "plt.plot(x, x+0, '-.k')\n",
    "plt.scatter(y, dtregr.predict(X))\n",
    "plt.title(\"Decision Tree Regressor\")\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,1200)\n",
    "plt.savefig('DTScatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_2 = dtregr.predict(X) - y\n",
    "plt.hist(error_2, edgecolor='black', linewidth = 1.0, rwidth = 0.9, bins=20)\n",
    "plt.title(\"Error plot for Decision Tree\")\n",
    "plt.xlabel(\"Predicted - Actual (MPa)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.savefig('DTError.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    An Artificial Neural Network (ANN) is an information processing paradigm that is inspired by the way biological nervous systems, such as the brain, process information. The key element of this paradigm is the novel structure of the information processing system. It is composed of a large number of highly interconnected processing elements working in unison to solve specific problems. ANNs, like people, learn by example. An ANN is configured for a specific application, such as pattern recognition or data classification, through a learning process. Learning in biological systems involves adjustments to the synaptic connections that exist between the neurones. This is true of ANNs as well.\n",
    "\n",
    "    Description of the ANN:\n",
    "\n",
    "    Layer (type)      |          Output Shape |            Param  \n",
    "\n",
    "    dense_1 (Dense)   |          (None, 512)  |            13312     \n",
    "    dense_2 (Dense)   |          (None, 256)  |            131328    \n",
    "    dense_3 (Dense)   |          (None, 128)  |            32896     \n",
    "    dense_4 (Dense)   |          (None, 64)   |            8256      \n",
    "    dense_5 (Dense)   |          (None, 01)   |            65        \n",
    "\n",
    "\n",
    "    Total params: 185,857\n",
    "\n",
    "    Trainable params: 185,857\n",
    "\n",
    "    Non-trainable params: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(512, kernel_initializer='normal',input_dim = 25, activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(64, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.fit(X_train, y_train, epochs=1000, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNerror = r2_score(y_train, NN_model.predict(X_train))\n",
    "print(\"Neural Network Training data:\", NNerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_2 = NN_model.predict(X_test)\n",
    "nnerror = r2_score(y_test, Y_pred_2)\n",
    "print(\"Neural Network Test data:\", nnerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the predictions made by the Artificial Neural Network and comparing it with the actual data points present in the training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_2 = NN_model.predict(X_train)\n",
    "plt.plot(range(349), train_pred_2)\n",
    "plt.plot(range(349), y_train)\n",
    "plt.ylim(0, 1500)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Neural Network on Training data')\n",
    "plt.savefig('NNTrain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_2 = NN_model.predict(X_test)\n",
    "plt.plot(range(88), Y_pred_2)\n",
    "plt.plot(range(88), y_test)\n",
    "plt.ylim(0, 1300)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Neural Network on Test data')\n",
    "plt.savefig('NNTest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1200, 1000)\n",
    "plt.plot(x, x+0, '-.k')\n",
    "plt.scatter(y, np.ravel(NN_model.predict(X)))\n",
    "plt.title(\"Neural Network\")\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,1200)\n",
    "plt.savefig('NNScatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_3 = np.ravel(NN_model.predict(X)) - y\n",
    "plt.hist(error_3, edgecolor='black', linewidth = 1.0, rwidth = 0.9, bins=20)\n",
    "plt.title(\"Error plot for Neural Network\")\n",
    "plt.xlabel(\"Predicted - Actual (MPa)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.savefig('NNError.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The random forest model is a type of additive model that makes predictions by combining decisions from a sequence of base models. More formally we can write this class of models as:\n",
    "\n",
    "                                        g(x)=f0(x)+f1(x)+f2(x)+...\n",
    "\n",
    "    where the final model g is the sum of simple base models fi. Here, each base classifier is a simple decision tree. This broad technique of using multiple models to obtain better predictive performance is called model ensembling. In random forests, all the base models are constructed independently using a different subsample of the data. The subsample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True\n",
    "\n",
    "    The random forest model is very good at handling tabular data with numerical features, or categorical features with fewer than hundreds of categories. Unlike linear models, random forests are able to capture non-linear interaction between the features and the target. When dealing with sparse input data (e.g. categorical features with large dimension), we can either pre-process the sparse features to generate numerical statistics, or switch to a linear model, which is better suited for such scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfregr = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=12, verbose=True, random_state=1, oob_score=True)\n",
    "rfregr.fit(X_train, y_train)\n",
    "rfrtrain = r2_score(y_train, rfregr.predict(X_train))\n",
    "print(\"Random Forest Train data:\", rfrtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_5 = rfregr.predict(X_test)\n",
    "RFRerror = r2_score(y_test, Y_pred_5)\n",
    "print(\"Random Forest Test data:\", RFRerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the predictions made by the Random Forest Regressor and comparing it with the actual data points present in the training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_5 = rfregr.predict(X_train)\n",
    "plt.plot(range(349), train_pred_5)\n",
    "plt.plot(range(349), y_train)\n",
    "plt.ylim(0, 1500)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Random Forest on Training data')\n",
    "plt.savefig('RFTrain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(88), Y_pred_5)\n",
    "plt.plot(range(88), y_test)\n",
    "plt.ylim(0, 1300)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Random Forest on Test data')\n",
    "plt.savefig('RFTest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the attribute 'feature_importances_' to rank the importance of each feature with respect to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = rfregr.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rfregr.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importance)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importance[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1200, 1000)\n",
    "plt.plot(x, x+0, '-.k')\n",
    "plt.scatter(y, rfregr.predict(X))\n",
    "plt.title(\"Random Forest\")\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,1200)\n",
    "plt.savefig('RFScatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_4 = rfregr.predict(X) - y\n",
    "plt.hist(error_4, edgecolor='black', linewidth = 1.0, rwidth = 0.9, bins=20)\n",
    "plt.title(\"Error plot for Random Forest\")\n",
    "plt.xlabel(\"Predicted - Actual (MPa)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.savefig('RFError.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. The objective of any supervised learning algorithm is to define a loss function and minimize it. Gradient Boosting allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbregr = GradientBoostingRegressor(n_estimators=100, max_depth=6, verbose=True, random_state=1)\n",
    "gbregr.fit(X_train, y_train)\n",
    "gbrtrain = r2_score(y_train, gbregr.predict(X_train))\n",
    "print(\"Random Forest Train data:\", gbrtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_6 = gbregr.predict(X_test)\n",
    "GBRerror = r2_score(y_test, Y_pred_6)\n",
    "print(\"Random Forest Test data:\", GBRerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the predictions made by the Gradient Boosting Regressor and comparing it with the actual data points present in the training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_6 = gbregr.predict(X_train)\n",
    "plt.plot(range(349), train_pred_6)\n",
    "plt.plot(range(349), y_train)\n",
    "plt.ylim(0, 1500)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Gradient Boosting on Training data')\n",
    "plt.savefig('GBTrain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(88), Y_pred_6)\n",
    "plt.plot(range(88), y_test)\n",
    "plt.ylim(0, 1300)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Gradient Boosting on Test data')\n",
    "plt.savefig('GBTest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1200, 1000)\n",
    "plt.plot(x, x+0, '-.k')\n",
    "plt.scatter(y, gbregr.predict(X))\n",
    "plt.title(\"Random Forest\")\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,1200)\n",
    "plt.savefig('GBScatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_5 = gbregr.predict(X) - y\n",
    "plt.hist(error_5, edgecolor='black', linewidth = 1.0, rwidth = 0.9, bins=20)\n",
    "plt.title(\"Error plot for Gradient Boosting\")\n",
    "plt.xlabel(\"Predicted - Actual (MPa)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.savefig('GBError.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADA Boost regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    An AdaBoost regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.\n",
    "\n",
    "    AdaBoost can be used to boost the performance of any machine learning algorithm. These are models that achieve accuracy just above random chance on a classification problem. The most suited and therefore most common algorithm used with AdaBoost are decision trees with one level. Because these trees are so short and only contain one decision for classification, they are often called decision stumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "adaregr = AdaBoostRegressor(n_estimators=100, learning_rate=0.9, random_state=1)\n",
    "adaregr.fit(X_train, y_train)\n",
    "adartrain = r2_score(y_train, adaregr.predict(X_train))\n",
    "print(\"Random Forest Train data:\", adartrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_7 = adaregr.predict(X_test)\n",
    "adaRerror = r2_score(y_test, Y_pred_7)\n",
    "print(\"Random Forest Test data:\", adaRerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the predictions made by the ADA Boost Regressor and comparing it with the actual data points present in the training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_7 = adaregr.predict(X_train)\n",
    "plt.plot(range(349), train_pred_7)\n",
    "plt.plot(range(349), y_train)\n",
    "plt.ylim(0, 1500)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of ADA Boosting on Training data')\n",
    "plt.savefig('ADATrain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(88), Y_pred_7)\n",
    "plt.plot(range(88), y_test)\n",
    "plt.ylim(0, 1300)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of ADA Boosting on Test data')\n",
    "plt.savefig('ADATest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_1 = adaregr.feature_importances_\n",
    "std_1 = np.std([tree.feature_importances_ for tree in adaregr.estimators_],\n",
    "             axis=0)\n",
    "indices_1 = np.argsort(importance_1)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(independent_features.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices_1[f], importance_1[indices_1[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1200, 1000)\n",
    "plt.plot(x, x+0, '-.k')\n",
    "plt.scatter(y, adaregr.predict(X))\n",
    "plt.title(\"ADA Boost\")\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,1200)\n",
    "plt.savefig('ADAScatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_6 = adaregr.predict(X) - y\n",
    "plt.hist(error_6, edgecolor='black', linewidth = 1.0, rwidth = 0.9, bins=20)\n",
    "plt.title(\"Error plot for ADA Boosting\")\n",
    "plt.xlabel(\"Predicted - Actual (MPa)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.savefig('ADAError.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with faster training speed and higher efficiency, lower memory usage and better accuracy. It is based on decision tree algorithms and is used for ranking, classification, regression and many more maching learning tasks. \n",
    "    \n",
    "    Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "lgbmregr = lgb.LGBMRegressor(n_jobs=-1, subsample=1.0, learning_rate=0.5, min_split_gain=.01)\n",
    "lgbmregr.fit(X_train, y_train)\n",
    "lgbmrtrain = r2_score(y_train, lgbmregr.predict(X_train))\n",
    "print(\"Light GBM Train data:\", lgbmrtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmrtrain = r2_score(y_train, lgbmregr.predict(X_train))\n",
    "print(\"Light GBM Train data:\", lgbmrtrain)\n",
    "Y_pred_8 = lgbmregr.predict(X_test)\n",
    "lgbmRerror = r2_score(y_test, Y_pred_8)\n",
    "print(\"Light GBM Test data:\", lgbmRerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the predictions made by the LightGBM Regressor and comparing it with the actual data points present in the training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_8 = lgbmregr.predict(X_train)\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(range(349), train_pred_8)\n",
    "plt.plot(range(349), y_train)\n",
    "plt.ylim(0, 1500)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Light GBM on Training data')\n",
    "plt.savefig('LGBMTrain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(88), Y_pred_8)\n",
    "plt.plot(range(88), y_test)\n",
    "plt.ylim(0, 1300)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Light GBM on Test data')\n",
    "plt.savefig('LGBMTest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the attribute 'feature_importances_' to rank the importance of each feature with respect to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\n",
    "feature_imp = pd.DataFrame(sorted(zip(lgbmregr.feature_importances_,independent_features.columns)), columns=['Value','Feature'])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('lgbm_importances-01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1200, 1000)\n",
    "plt.plot(x, x+0, '-.k')\n",
    "plt.scatter(y, lgbmregr.predict(X))\n",
    "plt.title(\"Light GBM\")\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,1200)\n",
    "plt.savefig('LGBMScatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_7 = lgbmregr.predict(X) - y\n",
    "plt.hist(error_7, edgecolor='black', linewidth = 1.0, rwidth = 0.9, bins=20)\n",
    "plt.title(\"Error plot for Light GBM\")\n",
    "plt.xlabel(\"Predicted - Actual (MPa)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 7)\n",
    "plt.savefig('LGBMError.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    XGBoost stands for “Extreme Gradient Boosting”, where the term “Gradient Boosting” originates from the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbregr = xgb.XGBRegressor(n_estimators=100, learning_rate=0.2, max_depth=4)\n",
    "xgbregr.fit(X_train, y_train)\n",
    "XGBtrain = r2_score(y_train, xgbregr.predict(X_train))\n",
    "print(\"XGBoost Regressor Train data:\", XGBtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_9 = xgbregr.predict(X_test)\n",
    "XGBerror = r2_score(y_test, Y_pred_9)\n",
    "print(\"XGBoost Regressor Test data:\", XGBerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the predictions made by the XGBoost Regressor and comparing it with the actual data points present in the training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_9 = xgbregr.predict(X_train)\n",
    "plt.plot(range(349), train_pred_9)\n",
    "plt.plot(range(349), y_train)\n",
    "plt.ylim(0, 1500)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of XGBoost on Training data')\n",
    "plt.savefig('XGBTrain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(88), Y_pred_9)\n",
    "plt.plot(range(88), y_test)\n",
    "plt.ylim(0, 1300)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of XGBoost on Test data')\n",
    "plt.savefig('XGBTest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "plot_importance(xgbregr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1200, 1000)\n",
    "plt.plot(x, x+0, '-.k')\n",
    "plt.scatter(y, xgbregr.predict(X))\n",
    "plt.title(\"XGBoost Regressor\")\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,1200)\n",
    "plt.savefig('XGBScatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_8 = xgbregr.predict(X) - y\n",
    "plt.hist(error_8, edgecolor='black', linewidth = 1.0, rwidth = 0.8, bins=20)\n",
    "plt.title(\"Error plot for XGBoost\")\n",
    "plt.xlabel(\"Predicted - Actual (MPa)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.savefig('XGBError.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an ensemble model to be used in fatigue prediction\n",
    "\n",
    "    The ensemble is trained on the top five features from the 25 features dataset and makes its predictions. The top five features are:\n",
    "    1. Tempering Temperature\n",
    "    2. % Carbon\n",
    "    3. % Chromium\n",
    "    4. % Manganese\n",
    "    5. % Phosphorous\n",
    "    \n",
    "    These five features have been chosen by the machine learning models and are backed by material science theory. Refer to the Thesis for more information regarding this.\n",
    "    The ensemble consists of the following models:\n",
    "    1. Neural Network\n",
    "    2. Decision Tree regressor\n",
    "    3. XGBoost regressor\n",
    "    4. LightGBM regressor\n",
    "    5. ADA Boost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['TT', 'C', 'Cr', 'Mn', 'P']\n",
    "X_final = data[features]\n",
    "y_final = data[target]\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_final, y_final, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model1 = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model1.add(Dense(512, kernel_initializer='normal',input_dim = 5, activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model1.add(Dense(128, kernel_initializer='normal',activation='relu'))\n",
    "NN_model1.add(Dense(64, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model1.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model1.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "NN_model1.fit(Xtrain, ytrain, epochs=400, validation_split = 0.2, callbacks=callbacks_list)\n",
    "dtregr.fit(Xtrain, ytrain)\n",
    "xgbregr.fit(Xtrain, ytrain)\n",
    "lgbmregr.fit(Xtrain, ytrain)\n",
    "adaregr.fit(Xtrain, ytrain)\n",
    "\n",
    "pred1 = np.ravel(NN_model1.predict(Xtest))\n",
    "pred2 = np.array(dtregr.predict(Xtest))\n",
    "pred3 = np.array(xgbregr.predict(Xtest))\n",
    "pred4 = np.array(lgbmregr.predict(Xtest))\n",
    "pred5 = np.array(adaregr.predict(Xtest))\n",
    "\n",
    "final_pred = np.mean(np.array([pred1, pred2, pred3, pred4, pred5]), axis=0)\n",
    "print(r2_score(final_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Models as pickle files to use for the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(NN_model1, open('model1.pkl','wb'))\n",
    "\n",
    "# pickle.dump(dtregr, open('model2.pkl','wb'))\n",
    "\n",
    "# pickle.dump(xgbregr, open('model3.pkl','wb'))\n",
    "\n",
    "# pickle.dump(lgbmregr, open('model4.pkl','wb'))\n",
    "\n",
    "# pickle.dump(adaregr, open('model5.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual model accuracy for 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svreg.fit(Xtrain, ytrain)\n",
    "svreg1 = r2_score(ytrain, svreg.predict(Xtrain))\n",
    "print(\"SVM Regression Train data:\", svreg1)\n",
    "Y_pred_17 = svreg.predict(Xtest)\n",
    "svregerror1 = r2_score(ytest, Y_pred_17)\n",
    "print(\"SVM Regression Test data:\", svregerror1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.fit(Xtrain, ytrain)\n",
    "linreg1 = r2_score(ytrain, linreg.predict(Xtrain))\n",
    "print(\"Linear Regression Train data:\", linreg1)\n",
    "Y_pred_18 = linreg.predict(Xtest)\n",
    "linregerror1 = r2_score(ytest, Y_pred_18)\n",
    "print(\"Linear Regression Test data:\", linregerror1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainpoly = poly.fit_transform(Xtrain) \n",
    "polreg.fit(XTrainpoly, ytrain)\n",
    "polreg1 = r2_score(ytrain, polreg.predict(XTrainpoly))\n",
    "print(\"Polynomial Regression Train data:\", polreg1)\n",
    "XTestpoly = poly.fit_transform(Xtest)\n",
    "Y_pred_19 = polreg.predict(XTestpoly)\n",
    "polregerror1 = r2_score(ytest, Y_pred_19)\n",
    "print(\"Polynomial Regression Test data:\", polregerror1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTRtrain1 = r2_score(ytrain, dtregr.predict(Xtrain))\n",
    "print(\"Decision Tree Train data:\", DTRtrain1)\n",
    "Y_pred_14 = dtregr.predict(Xtest)\n",
    "DTRerror1 = r2_score(ytest, Y_pred_14)\n",
    "print(\"Decision Tree Test data:\", DTRerror1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNerror_1 = r2_score(ytrain, NN_model1.predict(Xtrain))\n",
    "print(\"Neural Network Training data:\", NNerror_1)\n",
    "Y_pred_11 = NN_model1.predict(Xtest)\n",
    "nnerror_1 = r2_score(ytest, Y_pred_11)\n",
    "print(\"Neural Network Test data:\", nnerror_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfregr.fit(Xtrain, ytrain)\n",
    "rfregr1 = r2_score(ytrain, rfregr.predict(Xtrain))\n",
    "print(\"Random Forest  Train data:\", rfregr1)\n",
    "Y_pred_20 = rfregr.predict(Xtest)\n",
    "rfregrerror1 = r2_score(ytest, Y_pred_20)\n",
    "print(\"Random Forest  Test data:\", rfregrerror1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbregr.fit(Xtrain, ytrain)\n",
    "gbregr1 = r2_score(ytrain, gbregr.predict(Xtrain))\n",
    "print(\"Gradient Boosting Regression Train data:\", gbregr1)\n",
    "Y_pred_21 = gbregr.predict(Xtest)\n",
    "gbregrerror1 = r2_score(ytest, Y_pred_21)\n",
    "print(\"Gradient Boosting Regression Test data:\", gbregrerror1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adartrain1 = r2_score(ytrain, adaregr.predict(Xtrain))\n",
    "print(\"ADA Boost Train data:\", adartrain1)\n",
    "Y_pred_13 = adaregr.predict(Xtest)\n",
    "adaRerror1 = r2_score(ytest, Y_pred_13)\n",
    "print(\"ADA Boost Test data:\", adaRerror1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmrtrain1 = r2_score(ytrain, lgbmregr.predict(Xtrain))\n",
    "print(\"LightGBM Train data:\", lgbmrtrain1)\n",
    "Y_pred_15 = lgbmregr.predict(Xtest)\n",
    "lgbmRerror1 = r2_score(ytest, Y_pred_15)\n",
    "print(\"LightGBM Test data:\", lgbmRerror1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBtrain1 = r2_score(ytrain, xgbregr.predict(Xtrain))\n",
    "print(\"XGBoost Regressor Train data:\", XGBtrain1)\n",
    "Y_pred_12 = xgbregr.predict(Xtest)\n",
    "XGBerror1 = r2_score(ytest, Y_pred_12)\n",
    "print(\"XGBoost Regressor Test data:\", XGBerror1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction using the Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = (np.ravel(NN_model1.predict(X_final)) + np.array(dtregr.predict(X_final)) + np.array(xgbregr.predict(X_final)) + np.array(lgbmregr.predict(X_final)) + np.array(adaregr.predict(X_final)))/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the Ensemble Deep Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensembletrain = r2_score(y_train, train_pred_10)\n",
    "print(\"EnsembleDeep Learning model Train data:\", Ensembletrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensembleerror = r2_score(y_test, Y_pred_10)\n",
    "print(\"Ensemble Deep Learning model Test data:\", Ensembleerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the Accuracy plots and error plots for the Ensemble Deep Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_10 = (np.ravel(NN_model1.predict(Xtrain)) + np.array(dtregr.predict(Xtrain)) + np.array(xgbregr.predict(Xtrain)) + np.array(lgbmregr.predict(Xtrain)) + np.array(adaregr.predict(Xtrain)))/5\n",
    "plt.plot(range(349), train_pred_10)\n",
    "plt.plot(range(349), ytrain)\n",
    "plt.ylim(0, 1500)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "plt.xlabel('Data-points')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of Ensemble Deep Learning model on Training data')\n",
    "plt.savefig('EnsembleTrain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_10 = (np.ravel(NN_model1.predict(Xtest)) + np.array(dtregr.predict(Xtest)) + np.array(xgbregr.predict(Xtest)) + np.array(lgbmregr.predict(Xtest)) + np.array(adaregr.predict(Xtest)))/5\n",
    "plt.plot(range(88), Y_pred_10)\n",
    "plt.plot(range(88), y_test)\n",
    "plt.ylim(0, 1300)\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "plt.xlabel('Data-points')\n",
    "L = plt.legend('upper right', prop={'size': 12})\n",
    "L.get_texts()[0].set_text('predicted value')\n",
    "L.get_texts()[1].set_text('actual value')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 5)\n",
    "plt.title('Accuracy of ensemble Deep Learning model on Test data')\n",
    "plt.savefig('EnsembleTest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1200, 1000)\n",
    "plt.plot(x, x+0, '-.k')\n",
    "plt.scatter(y, final_pred)\n",
    "plt.title(\"Ensemble model\")\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,1200)\n",
    "plt.savefig('EnsembleScatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(437), final_pred, color='Blue', label='Prediction')\n",
    "plt.plot(range(437), y, color='orange', label='Actual')\n",
    "plt.title('Gauging the accuracy of the ensemble deep learning model')\n",
    "plt.legend(loc='upper left', prop={'size': 12})\n",
    "plt.xlabel('Data points')\n",
    "plt.ylabel('Fatigue Strength (MPa)')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 10)\n",
    "plt.savefig('EnsemblePred.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_9 = final_pred - y\n",
    "plt.hist(error_9, edgecolor='black', linewidth = 1.0, rwidth = 0.8, bins=20)\n",
    "plt.title(\"Error plot for Ensemble deep learning model\")\n",
    "plt.xlabel(\"Predicted - Actual (MPa)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 7)\n",
    "plt.savefig('EnsembleError.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
